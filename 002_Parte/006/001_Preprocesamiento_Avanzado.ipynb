{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FacuML/NLP/blob/main/001_Preprocesamiento_Avanzado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cuaderno Clase PLN - Preprocesamiento Avanzado\n",
        "\n",
        "**Objetivos:**\n",
        "*   Repasar la limpieza básica de texto.\n",
        "*   Entender y aplicar Stemming (NLTK).\n",
        "*   Entender y aplicar Lematización (spaCy).\n",
        "*   Comparar los resultados de ambas técnicas.\n",
        "*   Reflexionar sobre el impacto del preprocesamiento.\n",
        "\n",
        "**Agenda:**\n",
        "\n",
        "1.  Instalaciones e Importaciones\n",
        "2.  Repaso: Limpieza básica y Tokenización\n",
        "3.  El problema de las variantes de palabras\n",
        "4.  Stemming con NLTK\n",
        "5.  Lematización con spaCy\n",
        "6.  Comparación Stemming vs. Lematización\n",
        "7.  Micro-Laboratorio (Ejercicio Práctico)\n",
        "8.  Brainstorming"
      ],
      "metadata": {
        "id": "8_NHYb-fQu3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Instalaciones e Importaciones"
      ],
      "metadata": {
        "id": "iu4TosgXReRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar librerías (si no están ya en Colab)\n",
        "!pip install nltk spacy > /dev/null\n",
        "!python -m spacy download es_core_news_sm > /dev/null # Modelo pequeño de español para spaCy"
      ],
      "metadata": {
        "id": "2CyxzGvIRdxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías\n",
        "import nltk\n",
        "import spacy\n",
        "import re # Para expresiones regulares (limpieza)\n",
        "import string # Para signos de puntuación"
      ],
      "metadata": {
        "id": "YNOnXGQsRrII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "176_JS8VR2FR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b702c18c-2db6-41ea-9acd-9ebb1cf8e11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar modelo de spaCy en español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "print(\"Modelo de spaCy 'es_core_news_sm' cargado.\")"
      ],
      "metadata": {
        "id": "2oqkr1o2SCab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f2d35c-e242-4464-d3e6-28eb62098461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo de spaCy 'es_core_news_sm' cargado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar stopwords en español de NLTK\n",
        "stopwords_es = nltk.corpus.stopwords.words('spanish')"
      ],
      "metadata": {
        "id": "RHvRsNlySKbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Añadir algunas stopwords comunes si es necesario (opcional)\n",
        "# stopwords_es.extend(['tan', 'van', 'ser', 'haber', 'ir'])\n",
        "print(f\"\\nEjemplo de stopwords en español (NLTK): {stopwords_es[:15]}...\")"
      ],
      "metadata": {
        "id": "1Yq3GXkdSNqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c91c02c7-7cd7-4a70-a50b-593d3577dfbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ejemplo de stopwords en español (NLTK): ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Repaso: Limpieza básica y Tokenización\n",
        "\n",
        "Recordemos los pasos comunes que ya vimos:\n",
        "*   **Pasar a minúsculas:** Para tratar \"Hola\" y \"hola\" igual.\n",
        "*   **Quitar números:** A menudo no aportan significado general.\n",
        "*   **Quitar signos de puntuación:** Como ',', '.', '!', '?'.\n",
        "*   **Quitar stopwords:** Palabras muy comunes (\"el\", \"la\", \"de\", \"que\", \"y\"...) que aparecen mucho pero no suelen distinguir el tema del texto.\n",
        "*   **Tokenización:** Dividir el texto en unidades (palabras o \"tokens\")."
      ],
      "metadata": {
        "id": "2P1d3ILUlAC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de texto\n",
        "texto_ejemplo = \"Los niños corrían rápidamente por el parque, jugando y riendo. ¡Qué día más lindo!\""
      ],
      "metadata": {
        "id": "MbQyV_bJlKHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función simple de limpieza y tokenización (usando NLTK para stopwords y tokenización)\n",
        "def limpiar_tokenizar_basico(texto):\n",
        "  # 1. Minúsculas\n",
        "  texto = texto.lower()\n",
        "  # 2. Quitar números (usando expresiones regulares)\n",
        "  texto = re.sub(r'\\d+', '', texto)\n",
        "  # 3. Quitar puntuación\n",
        "  texto = texto.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
        "  # 4. Quitar espacios extra\n",
        "  texto = texto.strip()\n",
        "  # 5. Tokenizar\n",
        "  tokens = nltk.word_tokenize(texto, language='spanish')\n",
        "  # 6. Quitar stopwords\n",
        "  tokens_limpios = [palabra for palabra in tokens if palabra not in stopwords_es]\n",
        "  return tokens_limpios"
      ],
      "metadata": {
        "id": "DONcWdH2lNpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar la función\n",
        "tokens_basicos = limpiar_tokenizar_basico(texto_ejemplo)\n",
        "print(\"Texto original:\", texto_ejemplo)\n",
        "print(\"Tokens después de limpieza básica y quitar stopwords (NLTK):\", tokens_basicos)"
      ],
      "metadata": {
        "id": "DV-6TnQMlxTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8667b402-5344-4f1f-89be-a9f1530d15f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Los niños corrían rápidamente por el parque, jugando y riendo. ¡Qué día más lindo!\n",
            "Tokens después de limpieza básica y quitar stopwords (NLTK): ['niños', 'corrían', 'rápidamente', 'parque', 'jugando', 'riendo', 'día', 'lindo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. El problema de las variantes de palabras\n",
        "\n",
        "Observen los tokens resultantes: `['niños', 'corrían', 'rápidamente', 'parque', 'jugando', 'riendo', 'día', 'lindo']`.\n",
        "\n",
        "Tenemos \"corrían\", \"jugando\", \"riendo\". Si tuviéramos otro texto con \"correr\", \"juega\", \"reír\", serían tokens diferentes.\n",
        "\n",
        "**¿No sería útil agrupar las palabras que comparten una raíz o significado base?**\n",
        "\n",
        "*   **corrían, correr, corremos, corrió -> CORRER**\n",
        "*   **jugando, juega, jugamos -> JUGAR**\n",
        "\n",
        "Esto ayuda a:\n",
        "*   Reducir el tamaño del vocabulario (menos columnas en BoW/TF-IDF).\n",
        "*   Agrupar conceptos similares.\n",
        "\n",
        "Dos técnicas principales para esto: **Stemming** y **Lematización**."
      ],
      "metadata": {
        "id": "NcrJG67umD0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Stemming con NLTK\n",
        "\n",
        "*   **¿Qué es?** Un proceso **heurístico** (basado en reglas simples) para cortar el final de las palabras y obtener su \"raíz\" o \"stem\".\n",
        "*   **No siempre produce una palabra real** del diccionario.\n",
        "*   **Ventajas:** Rápido, simple, reduce mucho el vocabulario.\n",
        "*   **Desventajas:** A veces \"corta\" demasiado o agrupa palabras incorrectamente. No considera el contexto gramatical.\n",
        "*   **Herramienta:** NLTK tiene `SnowballStemmer` para español."
      ],
      "metadata": {
        "id": "y1n0lY4-mQLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Crear un stemmer para español\n",
        "stemmer = SnowballStemmer('spanish')"
      ],
      "metadata": {
        "id": "WCrfk44pmEUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a aplicar stemming a los tokens que obtuvimos antes (después de limpieza básica)\n",
        "stems_nltk = [stemmer.stem(token) for token in tokens_basicos]\n",
        "\n",
        "print(\"Tokens originales (limpios):\", tokens_basicos)\n",
        "print(\"Stems resultantes (NLTK): \", stems_nltk)"
      ],
      "metadata": {
        "id": "3P_DAqeImfYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3cbc54-dbf7-403e-b6ac-b1a69fcf2b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens originales (limpios): ['niños', 'corrían', 'rápidamente', 'parque', 'jugando', 'riendo', 'día', 'lindo']\n",
            "Stems resultantes (NLTK):  ['niñ', 'corr', 'rapid', 'parqu', 'jug', 'riend', 'dia', 'lind']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Probemos con otras palabras relacionadas\n",
        "palabras_relacionadas = ['correr', 'corría', 'corriendo', 'corredor', 'corredores']\n",
        "stems_relacionadas = [stemmer.stem(p) for p in palabras_relacionadas]\n",
        "print(f\"\\nStemming para {palabras_relacionadas}: {stems_relacionadas}\") # Notar que agrupa bien"
      ],
      "metadata": {
        "id": "MLFpz2X_mbJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508bb44d-9b42-43be-af55-5051a01de652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming para ['correr', 'corría', 'corriendo', 'corredor', 'corredores']: ['corr', 'corr', 'corr', 'corredor', 'corredor']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "palabras_problematicas = ['computadora', 'computación', 'computar']\n",
        "stems_problematicos = [stemmer.stem(p) for p in palabras_problematicas]\n",
        "print(f\"Stemming para {palabras_problematicas}: {stems_problematicos}\") # Funciona razonable"
      ],
      "metadata": {
        "id": "HopByEBZmlkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c18af78e-37f0-4b76-ef1c-eb2e81cbb550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming para ['computadora', 'computación', 'computar']: ['comput', 'comput', 'comput']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "palabras_problematicas_2 = ['universidad', 'universo']\n",
        "stems_problematicos_2 = [stemmer.stem(p) for p in palabras_problematicas_2]\n",
        "print(f\"Stemming para {palabras_problematicas_2}: {stems_problematicos_2}\") # ¡Ojo! Puede agrupar de más"
      ],
      "metadata": {
        "id": "oyNFRHInmm7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6552d376-004a-492a-9593-35ce531bde7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming para ['universidad', 'universo']: ['univers', 'univers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Lematización con spaCy\n",
        "\n",
        "*   **¿Qué es?** Un proceso más **lingüístico**, basado en diccionarios y análisis morfológico, para encontrar la forma canónica o de diccionario de una palabra (su \"lema\").\n",
        "*   **Produce palabras reales**.\n",
        "*   **Ventajas:** Más preciso conceptualmente, mejor para análisis semántico.\n",
        "*   **Desventajas:** Más lento computacionalmente, requiere modelos lingüísticos (como los de spaCy).\n",
        "*   **Herramienta:** spaCy lo hace automáticamente al procesar el texto con un modelo cargado (`nlp()`). El lema está en el atributo `token.lemma_`. spaCy también identifica stopwords (`token.is_stop`)."
      ],
      "metadata": {
        "id": "o0JXR2rRnIa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usaremos spaCy directamente sobre el texto original limpio (sin quitar stopwords aún)\n",
        "# porque spaCy necesita el contexto para lematizar bien.\n",
        "\n",
        "def limpiar_texto_spacy(texto):\n",
        "  # 1. Minúsculas\n",
        "  texto = texto.lower()\n",
        "  # 2. Quitar números\n",
        "  texto = re.sub(r'\\d+', '', texto)\n",
        "  # 3. Quitar puntuación (dejamos espacios)\n",
        "  texto = texto.translate(str.maketrans(string.punctuation + '¡¿', ' ' * len(string.punctuation + '¡¿')))\n",
        "  # 4. Quitar espacios extra\n",
        "  texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "  return texto"
      ],
      "metadata": {
        "id": "1RVojUlInWZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_limpio_spacy = limpiar_texto_spacy(texto_ejemplo)\n",
        "print(\"Texto limpio para spaCy:\", texto_limpio_spacy)"
      ],
      "metadata": {
        "id": "D4lS9v95ndWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51cf6805-8f16-4ed6-ab5f-bfce88636672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto limpio para spaCy: los niños corrían rápidamente por el parque jugando y riendo qué día más lindo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Procesar el texto limpio con spaCy\n",
        "doc = nlp(texto_limpio_spacy)"
      ],
      "metadata": {
        "id": "448foMWQnitz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener los lemas, filtrando stopwords y tokens no alfabéticos\n",
        "lemas_spacy = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "print(\"\\nLemas resultantes (spaCy, filtrando stopwords y no alfabéticos):\", lemas_spacy)"
      ],
      "metadata": {
        "id": "fz4UMjZJnkaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c12c9a-3e98-4f83-bde1-3d7f3ee23eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemas resultantes (spaCy, filtrando stopwords y no alfabéticos): ['niño', 'correr', 'rápidamente', 'parque', 'jugar', 'reir', 'lindo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos los lemas de las palabras relacionadas\n",
        "doc_relacionadas = nlp(\"correr corría corriendo corredor corredores\")\n",
        "lemas_relacionadas_spacy = [token.lemma_ for token in doc_relacionadas]\n",
        "print(f\"\\nLemas para {' '.join([t.text for t in doc_relacionadas])}: {lemas_relacionadas_spacy}\") # ¡Excelente! \"corredor\" es distinto de \"correr\""
      ],
      "metadata": {
        "id": "K_COplXgnugY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75f6894-92bd-4638-bbe4-e2a8e870ae26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemas para correr corría corriendo corredor corredores: ['correr', 'correr', 'correr', 'corredor', 'corredor']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_problematicas = nlp(\"computadora computación computar\")\n",
        "lemas_problematicas_spacy = [token.lemma_ for token in doc_problematicas]\n",
        "print(f\"Lemas para {' '.join([t.text for t in doc_problematicas])}: {lemas_problematicas_spacy}\") # \"computación\" se mantiene"
      ],
      "metadata": {
        "id": "yC98_51nnJhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b095558c-9863-4e8e-f395-2a4c97ce1acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemas para computadora computación computar: ['computadoro', 'computación', 'computar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_problematicas_2 = nlp(\"universidad universo\")\n",
        "lemas_problematicas_2_spacy = [token.lemma_ for token in doc_problematicas_2]\n",
        "print(f\"Lemas para {' '.join([t.text for t in doc_problematicas_2])}: {lemas_problematicas_2_spacy}\") # Correctamente separados"
      ],
      "metadata": {
        "id": "v1NJHViin78w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad9204e-9bfb-43ce-e027-fecee38bdc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemas para universidad universo: ['universidad', 'universo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Comparación Stemming vs. Lematización\n",
        "\n",
        "Veamos lado a lado los resultados para nuestro texto de ejemplo:\n",
        "\n",
        "*   **Tokens originales (limpios):** `['niños', 'corrían', 'rápidamente', 'parque', 'jugando', 'riendo', 'día', 'lindo']`\n",
        "*   **Stems (NLTK):** `['niñ', 'corr', 'rapid', 'parqu', 'jug', 'riend', 'dia', 'lind']`\n",
        "*   **Lemas (spaCy):** `['niño', 'correr', 'rápidamente', 'parque', 'jugar', 'reír', 'día', 'lindo']`\n",
        "\n",
        "**Observaciones:**\n",
        "*   Los lemas son palabras reales, los stems no siempre.\n",
        "*   La lematización parece capturar mejor la forma base (\"correr\", \"jugar\", \"reír\").\n",
        "*   El stemming es más agresivo (\"niñ\", \"corr\", \"rapid\").\n",
        "*   Ambos acortan \"día\" (sin tilde) de forma similar en este caso (NLTK por stem, spaCy porque el modelo puede no tener la tilde en su lema base).\n",
        "\n",
        "**¿Cuándo usar cuál?**\n",
        "*   **Stemming:** Cuando la velocidad es crucial y no importa tanto la interpretabilidad (ej: recuperación de información a gran escala).\n",
        "*   **Lematización:** Cuando la precisión semántica y la interpretabilidad son importantes (ej: análisis de sentimiento, clasificación de temas, chatbots). **Generalmente preferido si los recursos computacionales lo permiten.**"
      ],
      "metadata": {
        "id": "7Py5ywppoQ-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Micro-Laboratorio (Ejercicio Práctico)\n",
        "\n",
        "**Consigna:**\n",
        "\n",
        "Dado el siguiente conjunto de frases (reviews de películas):\n",
        "1.  Definir una función `preprocesar_nltk(texto)` que:\n",
        "    *   Limpie el texto (minúsculas, números, puntuación).\n",
        "    *   Tokenice.\n",
        "    *   Quite stopwords (usando la lista de NLTK).\n",
        "    *   Aplique Stemming (con `SnowballStemmer`).\n",
        "    *   Devuelva la lista de stems.\n",
        "2.  Definir una función `preprocesar_spacy(texto)` que:\n",
        "    *   Limpie el texto (minúsculas, números, puntuación - cuidado con no quitar espacios necesarios para spaCy).\n",
        "    *   Procese el texto con `nlp()`.\n",
        "    *   Devuelva la lista de lemas de los tokens que no sean stopwords (`token.is_stop`) y sean alfabéticos (`token.is_alpha`).\n",
        "3.  Aplicar ambas funciones a cada frase del dataset `reviews`.\n",
        "4.  Imprimir los resultados de ambas funciones para cada frase, uno debajo del otro, para poder comparar.\n",
        "5.  **Observar:** ¿Qué diferencias notables encuentran? ¿En qué casos un método parece funcionar \"mejor\" que el otro?"
      ],
      "metadata": {
        "id": "VMhtHWA5oovZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset para el ejercicio\n",
        "reviews = [\n",
        "    \"Una película emocionante con actuaciones brillantes. ¡Me encantó!\",\n",
        "    \"Muy aburrida y lenta. El guión era predecible y los actores no convencían.\",\n",
        "    \"Los efectos especiales fueron impresionantes, pero la historia dejaba mucho que desear.\",\n",
        "    \"¡Qué gran comedia! Me reí sin parar durante toda la película.\",\n",
        "    \"Un documental necesario que aborda temas importantes con profundidad y sensibilidad.\"\n",
        "]"
      ],
      "metadata": {
        "id": "57JZ3yh6oSDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .1 Procesar con NLTK las reviews (Steeming).\n"
      ],
      "metadata": {
        "id": "VH6PM0ZJo3bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def procesar_nltk(lista_reviews):\n",
        "  stemmer = SnowballStemmer('spanish')\n",
        "  all_stems_reviews=[]\n",
        "  for review in lista_reviews:\n",
        "    texto= review.lower()\n",
        "    texto = re.sub(r'\\d+', '', texto)\n",
        "    texto = texto.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
        "    texto = texto.strip()\n",
        "    tokens = nltk.word_tokenize(texto, language='spanish')\n",
        "    tokens_limpios = [palabras for palabras in tokens if palabras not in stopwords_es]\n",
        "    stems = [stemmer.stem(token)for token in tokens_limpios]\n",
        "    all_stems_reviews.append(stems)\n",
        "  return all_stems_reviews"
      ],
      "metadata": {
        "id": "c2BB6RbWpHA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(procesar_nltk(reviews))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFmzQ91sre1X",
        "outputId": "858138c4-e5b3-49fd-df9a-fb8370396289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['pelicul', 'emocion', 'actuacion', 'brillant', 'encant'], ['aburr', 'lent', 'guion', 'predec', 'actor', 'convenc'], ['efect', 'especial', 'impresion', 'histori', 'dej', 'des'], ['gran', 'comedi', 'rei', 'par', 'tod', 'pelicul'], ['documental', 'necesari', 'abord', 'tem', 'import', 'profund', 'sensibil']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .2 Procesar con Spacy las reviews (Lematisation)."
      ],
      "metadata": {
        "id": "_wVThYAzrRvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def procesar_spacy(lista_reviews):\n",
        "  all_stems_reviews=[]\n",
        "  for review in lista_reviews:\n",
        "    texto= review.lower()\n",
        "    texto = re.sub(r'\\d+', '', texto)\n",
        "    texto = texto.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
        "    texto = texto.strip()\n",
        "    doc = nlp(texto)\n",
        "    lemas = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "    all_stems_reviews.append(lemas)\n",
        "  return all_stems_reviews"
      ],
      "metadata": {
        "id": "e8KI41vAwqNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(procesar_spacy(reviews))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Zs6jjTgx3Jw",
        "outputId": "da82b7c5-9937-41da-dc8a-9ee60af990e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['película', 'emocionante', 'actuación', 'brillante', 'encantar'], ['aburrido', 'lento', 'guión', 'predecible', 'actor', 'convencer'], ['efecto', 'especial', 'impresionante', 'historia', 'dejar', 'desear'], ['comedia', 'reí', 'parar', 'película'], ['documental', 'necesario', 'abordar', 'tema', 'importante', 'profundidad', 'sensibilidad']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .4 Impresion y comparacion de ambas funciones"
      ],
      "metadata": {
        "id": "kQ1nDGWXyNdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Reviews originales:\")\n",
        "for review in reviews:\n",
        "  print(review)\n",
        "print(\"\\nStems (NLTK):\")\n",
        "for stems in procesar_nltk(reviews):\n",
        "  print(stems)\n",
        "print(\"\\nLemas (spaCy):\")\n",
        "for lemas in procesar_spacy(reviews):\n",
        "  print(lemas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk0zWSHpyKg9",
        "outputId": "79a0c257-4d55-4894-a1c5-33a3ea904d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reviews originales:\n",
            "Una película emocionante con actuaciones brillantes. ¡Me encantó!\n",
            "Muy aburrida y lenta. El guión era predecible y los actores no convencían.\n",
            "Los efectos especiales fueron impresionantes, pero la historia dejaba mucho que desear.\n",
            "¡Qué gran comedia! Me reí sin parar durante toda la película.\n",
            "Un documental necesario que aborda temas importantes con profundidad y sensibilidad.\n",
            "\n",
            "Stems (NLTK):\n",
            "['pelicul', 'emocion', 'actuacion', 'brillant', 'encant']\n",
            "['aburr', 'lent', 'guion', 'predec', 'actor', 'convenc']\n",
            "['efect', 'especial', 'impresion', 'histori', 'dej', 'des']\n",
            "['gran', 'comedi', 'rei', 'par', 'tod', 'pelicul']\n",
            "['documental', 'necesari', 'abord', 'tem', 'import', 'profund', 'sensibil']\n",
            "\n",
            "Lemas (spaCy):\n",
            "['película', 'emocionante', 'actuación', 'brillante', 'encantar']\n",
            "['aburrido', 'lento', 'guión', 'predecible', 'actor', 'convencer']\n",
            "['efecto', 'especial', 'impresionante', 'historia', 'dejar', 'desear']\n",
            "['comedia', 'reí', 'parar', 'película']\n",
            "['documental', 'necesario', 'abordar', 'tema', 'importante', 'profundidad', 'sensibilidad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Brainstorming\n",
        "\n",
        "Ahora que conocemos estas técnicas, pensemos:\n",
        "\n",
        "**¿Cómo podemos preprocesar el texto de manera que se eviten sesgos y discriminaciones?**\n",
        "\n",
        "*   ¿Qué pasa si la lista de `stopwords` que usamos (sea de NLTK o spaCy) quita palabras importantes para un grupo minoritario o en un contexto específico (ej: jerga, términos culturales)?\n",
        "*   ¿Los stemmers o lematizadores funcionan igual de bien con diferentes dialectos del español o con lenguaje inclusivo? (Probablemente no, los modelos estándar están entrenados en textos más \"formales\").\n",
        "*   Si quitamos nombres propios o entidades, ¿podríamos estar eliminando información crucial sobre representación?\n",
        "*   Al elegir agresividad (stemming) vs. precisión (lematización), ¿podríamos afectar diferencialmente el análisis de textos de distintos grupos?\n",
        "*   ¿Qué responsabilidad tenemos al elegir y aplicar estas técnicas? ¿Deberíamos documentar siempre nuestras decisiones de preprocesamiento?\n",
        "\n",
        "**(Discusión en grupo)**"
      ],
      "metadata": {
        "id": "Av-E8AxOpFUP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jKBXPxy-pImh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
